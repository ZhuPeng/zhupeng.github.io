---
layout: post
title: GitHub 开源项目 THUDM/CogVLM 介绍，a state-of-the-art-level open visual language model | 多模态预训练模型
tags: Python
---

大家好，又见面了，我是 GitHub 精选君！

今天要给大家推荐一个 GitHub 开源项目 THUDM/CogVLM，该项目在 GitHub 有超过 2.2k Star，用一句话介绍该项目就是：“a state-of-the-art-level open visual language model | 多模态预训练模型”。


![LLAVA Comparision](https://raw.githubusercontent.com/THUDM/CogVLM/master/assets/llava-comparison-min.png)
![compare](https://raw.githubusercontent.com/THUDM/CogVLM/master/assets/compare.png)
![Chat Examples](https://raw.githubusercontent.com/THUDM/CogVLM/master/assets/chat.png)



背景介绍：
在当前复杂的视觉和语言任务中，我们需要開发一个模型，能够精准地对图像内容进行描述，并能理解并回答各种类型的问题，为此，清华大学自然语言处理与社会人文计算实验室介绍了一款开源技术项目 CogVLM。

项目介绍：
[ CogVLM](https://github.com/THUDM/CogVLM) 是一款开源的视觉语言模型，它設有 100 亿视觉参数和 70 亿语言参数。CogVLM 在 10 项经典的跨模态基准测试中，如 NoCaps、Flicker30k captioning、RefCOCO、RefCOCO+、RefCOCOg、Visual7W、GQA、ScienceQA、VizWiz VQA 和 TDIUC 上都达到了创新性的性能，超过或等同于 PaLI-X 55B，并在 VQAv2、OKVQA、TextVQA、COCO captioning 等多项测试中名列第二。CogVLM 的核心功能包括可根据图像内容精确描述，并理解并回答各种类型的问题，擅长对图像的细节进行描述，偏差较少，能夠更精確抓住和理解图像的複雑以及仔細的內容。

如何使用：
您可以轻松在本地进行安装并使用：首先需要安装相应的依赖，后续我们提供了两个图形用户界面，分别是网页演示和命令行界面，你可以根据自己的喜好选择。如需在 Python 代码中使用，亦可以轻松修改CLI脚本以满足你的需求。

项目推介：
CogVLM 是一个开源的高效实用的模型，该项目的开发活跃，文档齐全，代码质量高，社区活跃，已经在许多知名公司和项目中得到应用，并得到了业界的一致好评和推荐。这个项目将会是你深入理解和学习视觉语言模型的极好选择，欢迎大家来使用和提出宝贵的建设性意见。


以下是该项目 Star 趋势图（代表项目的活跃程度）：

![](https://api.star-history.com/svg?repos=THUDM/CogVLM&type=Timeline)

更多项目详情请查看如下链接。

开源项目地址：https://github.com/THUDM/CogVLM 

开源项目作者：THUDM

以下是参与项目建设的所有成员：

![](https://contrib.rocks/image?repo=THUDM/CogVLM)

关注我们，一起探索有意思的开源项目。

